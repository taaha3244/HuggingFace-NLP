{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLpByYr4DkWc9oFFIo7oRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83fdb75af4b94a8c9378d8378687cb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_781ea0380d49440498324d9dc640e476",
              "IPY_MODEL_3e4ce5f7505b456fad0a7ef48b2fdbdb",
              "IPY_MODEL_af7b16f1066b44cd95d863e73fd54fa0"
            ],
            "layout": "IPY_MODEL_6aca53b20bbb4166b84aea36457b7a9f"
          }
        },
        "781ea0380d49440498324d9dc640e476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6072a28d0ca74397a8a42b3e20890aed",
            "placeholder": "​",
            "style": "IPY_MODEL_edba91172d104322b8a310168ca8c6a8",
            "value": "100%"
          }
        },
        "3e4ce5f7505b456fad0a7ef48b2fdbdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_265537bf8a7b4693967abc46f94c21ca",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c0cf80cf032400e91e14a15eada8afe",
            "value": 3
          }
        },
        "af7b16f1066b44cd95d863e73fd54fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4ef7af3bd54705b6981145a2bf6b74",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0cc7a2b49a473ba723bb5f04d3748c",
            "value": " 3/3 [00:00&lt;00:00, 89.63it/s]"
          }
        },
        "6aca53b20bbb4166b84aea36457b7a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6072a28d0ca74397a8a42b3e20890aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edba91172d104322b8a310168ca8c6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "265537bf8a7b4693967abc46f94c21ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c0cf80cf032400e91e14a15eada8afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f4ef7af3bd54705b6981145a2bf6b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0cc7a2b49a473ba723bb5f04d3748c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taaha3244/HuggingFace-NLP/blob/main/Prompt_Engineering_In_context_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "!pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1WhUzKaErA",
        "outputId": "4b1df4a6-909d-4976-82b6-9abca05325de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "R2gBGtLpaeWy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
        "dataset=load_dataset(huggingface_dataset_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "83fdb75af4b94a8c9378d8378687cb55",
            "781ea0380d49440498324d9dc640e476",
            "3e4ce5f7505b456fad0a7ef48b2fdbdb",
            "af7b16f1066b44cd95d863e73fd54fa0",
            "6aca53b20bbb4166b84aea36457b7a9f",
            "6072a28d0ca74397a8a42b3e20890aed",
            "edba91172d104322b8a310168ca8c6a8",
            "265537bf8a7b4693967abc46f94c21ca",
            "3c0cf80cf032400e91e14a15eada8afe",
            "0f4ef7af3bd54705b6981145a2bf6b74",
            "3f0cc7a2b49a473ba723bb5f04d3748c"
          ]
        },
        "id": "b3qlSpcmbynN",
        "outputId": "d70efc11-667d-4758-ed03-1df7626516da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83fdb75af4b94a8c9378d8378687cb55"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices=[40]\n",
        "dash_line=\"-\".join('' for x in range(100) )\n",
        "\n",
        "for i,index in enumerate(example_indices):\n",
        "  print(dash_line)\n",
        "  print(\"Example\",i + 1)\n",
        "  print(dash_line)\n",
        "  print('INPUT DIALOGUE:')\n",
        "  print(dataset['test'][index]['dialogue'])\n",
        "  print(dash_line)\n",
        "  print('BASE LINE HUMAN SUMMARY:')\n",
        "  print(dataset['test'][index]['summary'])\n",
        "  print(dash_line)\n",
        "  print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND6mKaXXcsAC",
        "outputId": "3eee031e-198b-455a-a39e-cdd4ef106c53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT DIALOGUE:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASE LINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-small'\n",
        "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "f5SKQUHkcSu2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(model_name,use_fast=True)"
      ],
      "metadata": {
        "id": "OR0EBGrxeIgk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence='My name is Taaha'\n",
        "sentence_encoded=tokenizer(sentence,return_tensors='pt')\n",
        "sentence_decoded=tokenizer.decode(\n",
        "    sentence_encoded['input_ids'][0],\n",
        "    skip_special_tokens=True\n",
        "\n",
        ")\n",
        "\n",
        "print('Encoded Sentence : ')\n",
        "print(sentence_encoded['input_ids'][0])\n",
        "print('\\nDecoded sentence :')\n",
        "print(sentence_decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LaooXp5e8LT",
        "outputId": "67f4e541-674d-4e7f-9bc9-e358f459952e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Sentence : \n",
            "tensor([ 499,  564,   19, 2067,    9, 1024,    1])\n",
            "\n",
            "Decoded sentence :\n",
            "My name is Taaha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,index in enumerate(example_indices):\n",
        "  dialogue=dataset['test'][index]['dialogue']\n",
        "  summary=dataset['test'][index]['summary']\n",
        "\n",
        "  inputs=tokenizer(dialogue,return_tensors='pt')\n",
        "  output=tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs['input_ids'],\n",
        "          max_new_tokens=50\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "  print(dash_line)\n",
        "  print('Example',i + 1)\n",
        "  print(dash_line)\n",
        "  print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "  print(dash_line)\n",
        "  print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "  print(dash_line)\n",
        "  print(f'Model Generation-Without prompt engineering:\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l25uEtADfm65",
        "outputId": "3a4d6e5b-2ced-4824-a7cb-067cb27c03b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Generation-Without prompt engineering:\n",
            "#Person1#: I'm sorry, but I'm not sure if it's a good time to catch the train.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,index in enumerate(example_indices):\n",
        "  dialogue=dataset['test'][index]['dialogue']\n",
        "  summary=dataset['test'][index]['summary']\n",
        "\n",
        "  prompt=f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "  \"\"\"\n",
        "\n",
        "  inputs=tokenizer(prompt,return_tensors='pt')\n",
        "  output=tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs['input_ids'],\n",
        "          max_new_tokens=50\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "  print(dash_line)\n",
        "  print('Example',i + 1)\n",
        "  print(dash_line)\n",
        "  print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "  print(dash_line)\n",
        "  print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "  print(dash_line)\n",
        "  print(f'Model Generation-Zero Shot:\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRV1YjTrmbfC",
        "outputId": "d4fb4837-7402-4ebd-b323-1a73d9cf1022"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Generation-Zero Shot:\n",
            "How long is it?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,index in enumerate(example_indices):\n",
        "  dialogue=dataset['test'][index]['dialogue']\n",
        "  summary=dataset['test'][index]['summary']\n",
        "\n",
        "  prompt=f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Explain what is going on in the above dialogues in detail\n",
        "  \"\"\"\n",
        "\n",
        "  inputs=tokenizer(prompt,return_tensors='pt')\n",
        "  output=tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs['input_ids'],\n",
        "          max_new_tokens=50\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "  print(dash_line)\n",
        "  print('Example',i + 1)\n",
        "  print(dash_line)\n",
        "  print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "  print(dash_line)\n",
        "  print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "  print(dash_line)\n",
        "  print(f'Model Generation-Zero Shot:\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlCET31moWiW",
        "outputId": "84d63d56-f1cc-4f58-ab88-b701ba1eae38"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Generation-Zero Shot:\n",
            "Tom is going to the railway station. He must catch the train.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full,example_index_to_summarize):\n",
        "  prompt=''\n",
        "  for i,index in enumerate(example_indices_full):\n",
        "    dialogue=dataset['test'][index]['dialogue']\n",
        "    summary=dataset['test'][index]['summary']\n",
        "\n",
        "    prompt+=f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Explain what is going on in the above dialogues in detail\n",
        "{summary}\n",
        "\"\"\"\n",
        "  dialogue=dataset['test'][example_index_to_summarize]['dialogue']\n",
        "  prompt+=f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Explain what is going on in the above dialogues in detail\n",
        "\"\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "uCA-3hTapwoM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full=[40]\n",
        "example_index_to_summarize=200\n",
        "\n",
        "example=make_prompt(example_indices_full,example_index_to_summarize)\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXRInhkfrcYr",
        "outputId": "b62ad3fb-4d82-4b1d-ec57-0bad1b980e47"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "Explain what is going on in the above dialogues in detail\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Explain what is going on in the above dialogues in detail\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary=dataset['test'][example_index_to_summarize]['summary']\n",
        "inputs=tokenizer(example,return_tensors='pt')\n",
        "output=tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs['input_ids'],\n",
        "          max_new_tokens=50\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "print(dash_line)\n",
        "print(f'Model Generation-One Shot:\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_Qw6dqqjUp",
        "outputId": "668d6267-d5e4-42c4-fa44-b37b226172da"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Generation-One Shot:\n",
            "#Person1#: I'm not sure what to expect from your software. I'd like to make a flyer and banner for advertising. I'd like to add a CD-ROM drive.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full1=[40,80,120]\n",
        "example_index_to_summarize=200\n",
        "\n",
        "example=make_prompt(example_indices_full1,example_index_to_summarize)\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1sPfifMs75F",
        "outputId": "5656d3f2-3dbb-4e01-8e28-59503b0cd191"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "Explain what is going on in the above dialogues in detail\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "Explain what is going on in the above dialogues in detail\n",
            "Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
            "#Person2#: Yes. Thank you very much. \n",
            "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
            "#Person2#: Oh, is it? \n",
            "#Person1#: Would you change it to a new one? \n",
            "#Person2#: Yes, certainly. You have the receipt? \n",
            "#Person1#: Yes, I do. \n",
            "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
            "#Person1#: Thank you so much. \n",
            "\n",
            "Explain what is going on in the above dialogues in detail\n",
            "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Explain what is going on in the above dialogues in detail\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary=dataset['test'][example_index_to_summarize]['summary']\n",
        "inputs=tokenizer(example,return_tensors='pt')\n",
        "output=tokenizer.decode(\n",
        "      model.generate(\n",
        "          inputs['input_ids'],\n",
        "          max_new_tokens=50\n",
        "      )[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "print(dash_line)\n",
        "print(f'Model Generation-Few Shot:\\n{output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvmc8bjPuYhP",
        "outputId": "290d197c-4f0f-4143-ffb1-72ef7b9ffbd2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Generation-Few Shot:\n",
            "#Person1#: I'm not sure what I would need. I'd like to upgrade my computer. I'd like to add a CD-ROM drive.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SpQwWXq2udLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}